---
# Source: prometheus/templates/alertmanager-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-alertmanager
data:
  alertmanager.yml: |
    global: {}
    receivers:
    - name: default-receiver
      webhook_configs:
      - send_resolved: true
        url: http://prometheus-am-relay.ops-sre.svc/webhook
    route:
      group_interval: 5m
      group_wait: 10s
      receiver: default-receiver
      repeat_interval: 3h
    
---
# Source: prometheus/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus
data:
  alerts: |
    groups:
    - name: blackbox-probes
      rules:
      - alert: probeFailures
        annotations:
          message: Probe {{  $labels.name }} failed
        expr: |
          probe_success{}==0
        for: 3m
        labels:
          severity: critical\n
    - name: kubernetes-absent
      rules:
      - alert: AlertmanagerDown
        annotations:
          message: Alertmanager has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerdown
        expr: |
          absent(up{component="alertmanager",namespace="ops-sre"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: CoreDNSDown
        annotations:
          message: CoreDNS has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-corednsdown
        expr: |
          absent(up{app="kube-dns"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: KubeAPIDown
        annotations:
          message: KubeAPI has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown
        expr: |
          absent(up{app="apiserver"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: KubeControllerManagerDown
        annotations:
          message: KubeControllerManager has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown
        expr: |
          absent(up{job="kube-controller-manager"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: KubeSchedulerDown
        annotations:
          message: KubeScheduler has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown
        expr: |
          absent(up{job="kube-scheduler"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: KubeStateMetricsDown
        annotations:
          message: KubeStateMetrics has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricsdown
        expr: |
          absent(up{job="kube-state-metrics"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: KubeletDown
        annotations:
          message: Kubelet has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown
        expr: |
          absent(up{job="kubelet"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: NodeExporterDown
        annotations:
          message: NodeExporter has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeexporterdown
        expr: |
          absent(up{app="node-exporter"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: PrometheusDown
        annotations:
          message: Prometheus has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusdown
        expr: |
          absent(up{app="prometheus",namespace="ops-sre"} == 1)
        for: 15m
        labels:
          severity: critical
    - name: kubernetes-apps
      rules:
      - alert: KubePodCrashLooping
        annotations:
          message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
            }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
        expr: |
          rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) * 60 * 5 > 0
        for: 1h
        labels:
          severity: critical
      - alert: KubePodNotReady
        annotations:
          message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
            state for longer than an hour.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
        expr: |
          sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) > 0
        for: 1h
        labels:
          severity: critical
      - alert: KubeDeploymentGenerationMismatch
        annotations:
          message: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
            }} does not match, this indicates that the Deployment has failed but has not
            been rolled back.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
        expr: |
          kube_deployment_status_observed_generation{job="kube-state-metrics"}
            !=
          kube_deployment_metadata_generation{job="kube-state-metrics"}
        for: 15m
        labels:
          severity: critical
      - alert: KubeDeploymentReplicasMismatch
        annotations:
          message: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
            matched the expected number of replicas for longer than an hour.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
        expr: |
          kube_deployment_spec_replicas{job="kube-state-metrics"}
            !=
          kube_deployment_status_replicas_available{job="kube-state-metrics"}
        for: 1h
        labels:
          severity: critical
      - alert: KubeStatefulSetReplicasMismatch
        annotations:
          message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not
            matched the expected number of replicas for longer than 15 minutes.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
        expr: |
          kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas{job="kube-state-metrics"}
        for: 15m
        labels:
          severity: critical
      - alert: KubeStatefulSetGenerationMismatch
        annotations:
          message: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
            }} does not match, this indicates that the StatefulSet has failed but has
            not been rolled back.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
        expr: |
          kube_statefulset_status_observed_generation{job="kube-state-metrics"}
            !=
          kube_statefulset_metadata_generation{job="kube-state-metrics"}
        for: 15m
        labels:
          severity: critical
      - alert: KubeStatefulSetUpdateNotRolledOut
        annotations:
          message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
            has not been rolled out.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
        expr: |
          max without (revision) (
            kube_statefulset_status_current_revision{job="kube-state-metrics"}
              unless
            kube_statefulset_status_update_revision{job="kube-state-metrics"}
          )
            *
          (
            kube_statefulset_replicas{job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
          )
        for: 15m
        labels:
          severity: critical
      - alert: KubeDaemonSetRolloutStuck
        annotations:
          message: Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
            }}/{{ $labels.daemonset }} are scheduled and ready.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
        expr: |
          kube_daemonset_status_number_ready{job="kube-state-metrics"}
            /
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
        for: 15m
        labels:
          severity: critical
      - alert: KubeDaemonSetNotScheduled
        annotations:
          message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are not scheduled.'
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
        expr: |
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
            -
          kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
        for: 10m
        labels:
          severity: warning
      - alert: KubeDaemonSetMisScheduled
        annotations:
          message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are running where they are not supposed to run.'
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
        expr: |
          kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
        for: 10m
        labels:
          severity: warning
      - alert: KubeCronJobRunning
        annotations:
          message: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
            than 1h to complete.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
        expr: |
          time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
        for: 1h
        labels:
          severity: warning
      - alert: KubeJobCompletion
        annotations:
          message: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than
            one hour to complete.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
        expr: |
          kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
        for: 1h
        labels:
          severity: warning
      - alert: KubeJobFailed
        annotations:
          message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
        expr: |
          kube_job_status_failed{job="kube-state-metrics"}  > 0
        for: 1h
        labels:
          severity: warning
    - name: kubernetes-resources
      rules:
      - alert: KubeCPUOvercommitPods
        annotations:
          message: Cluster has overcommitted CPU resource requests for Pods and cannot
            tolerate node failure.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
        expr: |
          sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
            /
          sum(node:node_num_cpu:sum)
            >
          (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
        for: 5m
        labels:
          severity: warning
      - alert: KubeMemOvercommitPods
        annotations:
          message: Cluster has overcommitted memory resource requests for Pods and cannot
            tolerate node failure.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
        expr: |
          sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
            /
          sum(node_memory_MemTotal_bytes)
            >
          (count(node:node_num_cpu:sum)-1)
            /
          count(node:node_num_cpu:sum)
        for: 5m
        labels:
          severity: warning
      - alert: KubeCPUOvercommitNamespaces
        annotations:
          message: Cluster has overcommitted CPU resource requests for Namespaces.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
        expr: |
          sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
            /
          sum(node:node_num_cpu:sum)
            > 1.5
        for: 5m
        labels:
          severity: warning
      - alert: KubeMemOvercommitNamespaces
        annotations:
          message: Cluster has overcommitted memory resource requests for Namespaces.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
        expr: |
          sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
            /
          sum(node_memory_MemTotal_bytes{job="node-exporter"})
            > 1.5
        for: 5m
        labels:
          severity: warning
      - alert: KubeQuotaExceeded
        annotations:
          message: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value
            }}% of its {{ $labels.resource }} quota.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
        expr: |
          100 * kube_resourcequota{job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
            > 90
        for: 15m
        labels:
          severity: warning
      - alert: CPUThrottlingHigh
        annotations:
          message: '{{ printf "%0.0f" $value }}% throttling of CPU in namespace {{ $labels.namespace
            }} for container {{ $labels.container_name }} in pod {{ $labels.pod }}.'
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
        expr: "100 * sum(increase(container_cpu_cfs_throttled_periods_total{container_name!=\"\",
          }[5m])) by (container_name, pod, namespace)\n  /\nsum(increase(container_cpu_cfs_periods_total{}[5m]))
          by (container_name, pod, namespace)\n  > 25 \n"
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-storage
      rules:
      - alert: KubePersistentVolumeUsageCritical
        annotations:
          message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }}
            in Namespace {{ $labels.namespace }} is only {{ printf "%0.2f" $value }}%
            free.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
        expr: |
          100 * kubelet_volume_stats_available_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
            < 3
        for: 1m
        labels:
          severity: critical
      - alert: KubePersistentVolumeFullInFourDays
        annotations:
          message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is expected to fill up within four
            days. Currently {{ printf "%0.2f" $value }}% is available.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
        expr: |
          100 * (
            kubelet_volume_stats_available_bytes{job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet"}
          ) < 15
          and
          predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
        for: 5m
        labels:
          severity: critical
      - alert: KubePersistentVolumeErrors
        annotations:
          message: The persistent volume {{ $labels.persistentvolume }} has status {{
            $labels.phase }}.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
        expr: |
          kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
        for: 5m
        labels:
          severity: critical
    - name: kubernetes-system
      rules:
      - alert: KubeNodeNotReady
        annotations:
          message: '{{ $labels.node }} has been unready for more than an hour.'
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
        expr: |
          kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        for: 1h
        labels:
          severity: warning
      - alert: KubeVersionMismatch
        annotations:
          message: There are {{ $value }} different semantic versions of Kubernetes components
            running.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
        expr: |
          count(count by (gitVersion) (label_replace(kubernetes_build_info{job!="kube-dns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
        for: 1h
        labels:
          severity: warning
      - alert: KubeClientErrorsTotal
        annotations:
          message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
            }}' is experiencing {{ printf "%0.0f" $value }}% errors.'
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
        expr: |
          (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
            /
          sum(rate(rest_client_requests_total[5m])) by (instance, job))
          * 100 > 1
        for: 15m
        labels:
          severity: warning
      - alert: KubeClientErrorsPerSecond
        annotations:
          message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
            }}'\n is experiencing {{ printf "%0.0f" $value }} errors / second.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
        expr: |
          sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
        for: 15m
        labels:
          severity: warning
      - alert: KubeletTooManyPods
        annotations:
          message: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close
            to the limit of 110.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
        expr: |
          kubelet_running_pod_count{job="kubelet"} > 110 * 0.9
        for: 15m
        labels:
          severity: warning
      - alert: KubeAPILatencyHigh
        annotations:
          message: The API server has a 99th percentile latency of {{ $value }} seconds
            for {{ $labels.verb }} {{ $labels.resource }}.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
        expr: |
          cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
        for: 10m
        labels:
          severity: warning
      - alert: KubeAPILatencyHigh
        annotations:
          message: The API server has a 99th percentile latency of {{ $value }} seconds
            for {{ $labels.verb }} {{ $labels.resource }}.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
        expr: |
          cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsCritial
        annotations:
          message: API server is returning errors for {{ $value }}% of requests.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m]))
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) * 100 > 3
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsWarning
        annotations:
          message: API server is returning errors for {{ $value }}% of requests.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m]))
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) * 100 > 1
        for: 10m
        labels:
          severity: warning
      - alert: KubeAPIErrorsByResoruceCritial
        annotations:
          message: API server is returning errors for {{ $value }}% of requests for {{
            $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) by (resource,subresource,verb) * 100 > 10
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsByResourceWarning
        annotations:
          message: API server is returning errors for {{ $value }}% of requests for {{
            $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
        expr: |
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) by (resource,subresource,verb) * 100 > 5
        for: 10m
        labels:
          severity: warning
      - alert: KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring
            in less than 7.0 days.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
        labels:
          severity: warning
      - alert: KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring
            in less than 24.0 hours.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
        labels:
          severity: critical
    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerConfigInconsistent
        annotations:
          message: The configuration of the instances of the Alertmanager cluster `{{$labels.service}}`
            are out of sync.
        expr: |
          count_values("config_hash", alertmanager_config_hash{job="alertmanager-main",namespace="ops-sre"}) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_spec_replicas{job="prometheus-operator",namespace="ops-sre",controller="alertmanager"}, "service", "alertmanager-$1", "name", "(.*)") != 1
        for: 5m
        labels:
          severity: critical
      - alert: AlertmanagerFailedReload
        annotations:
          message: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
            }}/{{ $labels.pod}}.
        expr: |
          alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="ops-sre"} == 0
        for: 10m
        labels:
          severity: warning
    - name: general.rules
      rules:
      - alert: TargetDown
        annotations:
          message: '{{ $value }}% of the {{ $labels.job }} targets are down.'
        expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10
        for: 10m
        labels:
          severity: warning
      - alert: Watchdog
        annotations:
          message: |
            This is an alert meant to ensure that the entire alerting pipeline is functional.
            This alert is always firing, therefore it should always be firing in Alertmanager
            and always fire against a receiver. There are integrations with various notification
            mechanisms that send a notification when this alert is not firing. For example the
            "DeadMansSnitch" integration in PagerDuty.
        expr: vector(1)
        labels:
          severity: none
    - name: kube-prometheus-node-alerting.rules
      rules:
      - alert: NodeDiskRunningFull
        annotations:
          message: Device {{ $labels.device }} of node-exporter {{ $labels.namespace }}/{{
            $labels.pod }} will be full within the next 24 hours.
        expr: |
          (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[6h], 3600 * 24) < 0)
        for: 30m
        labels:
          severity: warning
      - alert: NodeDiskRunningFull
        annotations:
          message: Device {{ $labels.device }} of node-exporter {{ $labels.namespace }}/{{
            $labels.pod }} will be full within the next 2 hours.
        expr: |
          (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[30m], 3600 * 2) < 0)
        for: 10m
        labels:
          severity: critical
    - name: node-time
      rules:
      - alert: ClockSkewDetected
        annotations:
          message: Clock skew detected on node-exporter {{ $labels.namespace }}/{{ $labels.pod
            }}. Ensure NTP is configured correctly on this host.
        expr: |
          abs(node_timex_offset_seconds{job="node-exporter"}) > 0.03
        for: 2m
        labels:
          severity: warning
    - name: node-network
      rules:
      - alert: NetworkReceiveErrors
        annotations:
          message: Network interface "{{ $labels.device }}" showing receive errors on
            node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        expr: |
          rate(node_network_receive_errs_total{job="node-exporter",device!~"veth.+"}[2m]) > 0
        for: 2m
        labels:
          severity: warning
      - alert: NetworkTransmitErrors
        annotations:
          message: Network interface "{{ $labels.device }}" showing transmit errors on
            node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        expr: |
          rate(node_network_transmit_errs_total{job="node-exporter",device!~"veth.+"}[2m]) > 0
        for: 2m
        labels:
          severity: warning
      - alert: NodeNetworkInterfaceFlapping
        annotations:
          message: Network interface "{{ $labels.device }}" changing it's up status often
            on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        expr: |
          changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
        for: 2m
        labels:
          severity: warning
    - name: prometheus.rules
      rules:
      - alert: PrometheusConfigReloadFailed
        annotations:
          description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
          summary: Reloading Prometheus' configuration failed
        expr: |
          prometheus_config_last_reload_successful{app="prometheus",namespace="ops-sre"} == 0
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusNotificationQueueRunningFull
        annotations:
          description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{
            $labels.pod}}
          summary: Prometheus' alert notification queue is running full
        expr: |
          predict_linear(prometheus_notifications_queue_length{app="prometheus",namespace="ops-sre"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{app="prometheus",namespace="ops-sre"}
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusErrorSendingAlerts
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
            $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alert from Prometheus
        expr: |
          rate(prometheus_notifications_errors_total{app="prometheus",namespace="ops-sre"}[5m]) / rate(prometheus_notifications_sent_total{app="prometheus",namespace="ops-sre"}[5m]) > 0.01
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusErrorSendingAlerts
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
            $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alerts from Prometheus
        expr: |
          rate(prometheus_notifications_errors_total{app="prometheus",namespace="ops-sre"}[5m]) / rate(prometheus_notifications_sent_total{app="prometheus",namespace="ops-sre"}[5m]) > 0.03
        for: 10m
        labels:
          severity: critical
      - alert: PrometheusNotConnectedToAlertmanagers
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected
            to any Alertmanagers
          summary: Prometheus is not connected to any Alertmanagers
        expr: |
          prometheus_notifications_alertmanagers_discovered{app="prometheus",namespace="ops-sre"} < 1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusTSDBReloadsFailing
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            reload failures over the last four hours.'
          summary: Prometheus has issues reloading data blocks from disk
        expr: |
          increase(prometheus_tsdb_reloads_failures_total{app="prometheus",namespace="ops-sre"}[2h]) > 0
        for: 12h
        labels:
          severity: warning
      - alert: PrometheusTSDBCompactionsFailing
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            compaction failures over the last four hours.'
          summary: Prometheus has issues compacting sample blocks
        expr: |
          increase(prometheus_tsdb_compactions_failed_total{app="prometheus",namespace="ops-sre"}[2h]) > 0
        for: 12h
        labels:
          severity: warning
      - alert: PrometheusTSDBWALCorruptions
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead
            log (WAL).'
          summary: Prometheus write-ahead log is corrupted
        expr: |
          prometheus_tsdb_wal_corruptions_total{app="prometheus",namespace="ops-sre"} > 0
        for: 4h
        labels:
          severity: warning
      - alert: PrometheusNotIngestingSamples
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting
            samples.
          summary: Prometheus isn't ingesting samples
        expr: |
          rate(prometheus_tsdb_head_samples_appended_total{app="prometheus",namespace="ops-sre"}[5m]) <= 0
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusTargetScrapesDuplicate
        annotations:
          description: '{{$labels.namespace}}/{{$labels.pod}} has many samples rejected
            due to duplicate timestamps but different values'
          summary: Prometheus has many samples rejected
        expr: |
          increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{app="prometheus",namespace="ops-sre"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
    
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 5m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
    - job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
    - job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      scrape_interval: 5m
      scrape_timeout: 30s
    
    # - job_name: 'prometheus-blackbox-exporter'
    #   metrics_path: /probe
    #   params:
    #     module: [http_2xx]
    #   static_configs:
    #     - targets:
    #       - https://example.com
    #   relabel_configs:
    #     - source_labels: [__address__]
    #       target_label: __param_target
    #     - source_labels: [__param_target]
    #       target_label: instance
    #     - target_label: __address__
    #       replacement: prometheus-blackbox-exporter:9115
    - job_name: stackdriver-prometheus-sidecar
      scrape_interval: 1m
      scrape_timeout: 10s
      metrics_path: /metrics
      scheme: http
      static_configs:
      - targets:
        - localhost:9091
    - job_name: 'blackbox-services'
      scrape_interval: 1m
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
        - targets:
          - http://authorisation-service.sit-bx/api/healthcheck
          labels:
            name: (OB) Authorisation Service SIT
        - targets:
          - http://authorisation-service.lion-bx/api/healthcheck
          labels:
            name: (OB) Authorisation Service Lion
        - targets:
          - http://authorisation-service.jackal-bx/api/healthcheck
          labels:
            name: (OB) Authorisation Service Jackal
        - targets:
          - http://authorisation-service.eagle-bx/api/healthcheck
          labels:
            name: (OB) Authorisation Service Eagle
        - targets:
          - http://entitlement-service.sit-bx/api/healthcheck
          labels:
            name: (OB) Entitlement Service SIT
        - targets:
          - http://entitlement-service.jackal-bx/api/healthcheck
          labels:
            name: (OB) Entitlement Service Jackal
        - targets:
          - http://entitlement-service.eagle-bx/api/healthcheck
          labels:
            name: (OB) Entitlement Service Eagle
        - targets:
          - http://entitlement-service.lion-bx/api/healthcheck
          labels:
            name: (OB) Entitlement Service Lion
        - targets:
          - http://exportservice.sit-bx/api/healthcheck
          labels:
            name: (OB) Export Service SIT
        - targets:
          - http://kong-proxy-service.sit-integration-api-gateway/health
          labels:
            name: (OB) Kong Proxy Service SIT
        - targets:
          - http://kong-proxy-service.lion-integration-api-gateway/health
          labels:
            name: (OB) Kong Proxy Service Lion
        - targets:
          - http://kong-proxy-service.jackal-integration-api-gateway/health
          labels:
            name: (OB) Kong Proxy Service Jackal
        - targets:
          - http://kong-proxy-service.eagle-integration-api-gateway/health
          labels:
            name: (OB) Kong Proxy Service Eagle
        - targets:
          - http://productservice-preprod.csp.gcpnp.anz/healthcheck/up
          labels:
            name: (CSP) Product Service Preprod
        - targets:
          - http://helloworld.sit-sre
          labels:
            name: (OB) Helloworld SIT
        - targets:
          - http://ob-ms-v1-accounts.sit-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-accounts SIT
        - targets:
          - http://ob-ms-v1-accounts.lion-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-accounts Lion
        - targets:
          - http://ob-ms-v1-accounts.jackal-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-accounts Jackal
        - targets:
          - http://ob-ms-v1-accounts.eagle-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-accounts Eagle
        - targets:
          - http://ob-ms-v1-balances.sit-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-balances SIT
        - targets:
          - http://ob-ms-v1-balances.lion-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-balances Lion
        - targets:
          - http://ob-ms-v1-balances.eagle-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-balances Eagle
        - targets:
          - http://ob-ms-v1-balances.jackal-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-balances Jackal
        - targets:
          - http://ob-ms-v1-customer.sit-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-customer SIT
        - targets:
          - http://ob-ms-v1-customer.lion-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-customer Lion
        - targets:
          - http://ob-ms-v1-customer.jackal-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-customer Jackal
        - targets:
          - http://ob-ms-v1-customer.eagle-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-customer Eagle
        - targets:
          - http://ob-ms-v1-direct-debits.sit-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-direct-debits SIT
        - targets:
          - http://ob-ms-v1-direct-debits.lion-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-direct-debits Lion
        - targets:
          - http://ob-ms-v1-direct-debits.jackal-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-direct-debits Jackal
        - targets:
          - http://ob-ms-v1-direct-debits.eagle-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-direct-debits Eagle
        - targets:
          - http://ob-ms-v1-payees.sit-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-payees SIT
        - targets:
          - http://ob-ms-v1-payees.lion-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-payees Lion
        - targets:
          - http://ob-ms-v1-payees.jackal-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-payees Jackal
        - targets:
          - http://ob-ms-v1-payees.eagle-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-payees Eagle
        - targets:
          - http://ob-ms-v1-transactions.sit-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-transactions SIT
        - targets:
          - http://ob-ms-v1-transactions.lion-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-transactions Lion
        - targets:
          - http://ob-ms-v1-transactions.jackal-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-transactions Jackal
        - targets:
          - http://ob-ms-v1-transactions.eagle-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-transactions Eagle
        - targets:
          - http://ob-ms-v1-scheduled-payments.sit-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-scheduled-payments	 SIT
        - targets:
          - http://ob-ms-v1-scheduled-payments.lion-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-scheduled-payments	 Lion
        - targets:
          - http://ob-ms-v1-scheduled-payments.jackal-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-scheduled-payments	 Jackal
        - targets:
          - http://ob-ms-v1-scheduled-payments.eagle-integration-mediation/health-check
          labels:
            name: (OB) ob-ms-v1-scheduled-payments	 Eagle
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: prometheus-blackbox-exporter:9115
      scheme: https
      tls_config:
        ca_file: /etc/istio-certs/root-cert.pem
        cert_file: /etc/istio-certs/cert-chain.pem
        key_file: /etc/istio-certs/key.pem
        insecure_skip_verify: true  # prometheus does not support secure naming.
    - job_name: 'blackbox-ops'
      scrape_interval: 1m
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
        - targets:
          - http://grafana.ops-sre/api/health
          labels:
            name: (OB) Grafana
        - targets:
          - http://kiali.ops-sre:20001/api
          labels:
            name: (OB) Kiali
        - targets:
          - http://jaeger-query.ops-sre:16686
          labels:
            name: (OB) Jaeger
        - targets:
          - http://ob-ingressgateway.ingress:15020/healthz/ready
          labels:
            name: (OB) Ingress Gateway
        - targets:
          - http://ob-egressgateway.egress:15020/healthz/ready
          labels:
            name: (OB) Egress Gateway
        - targets:
          - http://istio-citadel.istio-system:15014/version
          labels:
            name: (OB) Istio Citadel
        - targets:
          - http://istio-galley.istio-system:15014/version
          labels:
            name: (OB) Istio Galley
        - targets:
          - http://istio-pilot.istio-system:8080/ready
          labels:
            name: (OB) Istio Pilot
        - targets:
          - http://istio-policy.istio-system:15014/version
          labels:
            name: (OB) Istio Policy
        - targets:
          - http://istio-telemetry.istio-system:15014/version
          labels:
            name: (OB) Istio Telemetry
        - targets:
          - http://opa.opa:8181/health
          labels:
            name: (OB) OPA
        - targets:
          - http://sealed-secrets-controller.ops-sre:8080/healthz
          labels:
            name: (OB) Sealed Secrets
        - targets:
          - http://rbacsync.ops-sre/healthz
          labels:
            name: (OB) RBACSync
        - targets:
          - http://twistlock-console.ops-sre:8081/api/v1/_ping
          labels:
            name: (OB) Twistlock
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: prometheus-blackbox-exporter:9115
      scheme: https
      tls_config:
        ca_file: /etc/istio-certs/root-cert.pem
        cert_file: /etc/istio-certs/cert-chain.pem
        key_file: /etc/istio-certs/key.pem
        insecure_skip_verify: true  # prometheus does not support secure naming.
    - job_name: 'blackbox-on-prem'
      scrape_timeout: 1m
      scrape_interval: 1m
      metrics_path: /probe
      params:
        module: [http_2xx_long]
      static_configs:
        - targets:
          - http://cloud.b2bgwsit.service.test/jwks/sti/oidc/.well-known/jwks.json
          labels:
            name: (OB) B2B DP SIT
        - targets:
          - http://cloud.b2bgwsit02.service.test/jwks/sti/oidc/.well-known/jwks.json
          labels:
            name: (OB) B2B DP SIT02
        - targets:
          - http://cloud.b2bgwsit03.service.test/jwks/sti/oidc/.well-known/jwks.json
          labels:
            name: (OB) B2B DP SIT03
        - targets:
          - http://cloud.b2bgwqa.service.test/jwks/sti/oidc/.well-known/jwks.json
          labels:
            name: (OB) B2B DP QA
        - targets:
          - http://gwb2bunauthqa.service.test/system/jwk_uri
          labels:
            name: (OB) B2B IG QA
        - targets:
          - http://gwb2bunauthobit.service.test/system/jwk_uri
          labels:
            name: (OB) B2B IG Industry
        - targets:
          - http://gwb2bunauthsit2.service.test/system/jwk_uri
          labels:
            name: (OB) B2B IG SIT02
        - targets:
          - http://gwb2bunauthsit3.service.test/system/jwk_uri
          labels:
            name: (OB) B2B IG SIT03
        - targets:
          - http://gwb2bunauthst2.service.test/system/jwk_uri
          labels:
            name: (OB) B2B IG ST2
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: prometheus-blackbox-exporter:9115
      scheme: https
      tls_config:
        ca_file: /etc/istio-certs/root-cert.pem
        cert_file: /etc/istio-certs/cert-chain.pem
        key_file: /etc/istio-certs/key.pem
        insecure_skip_verify: true  # prometheus does not support secure naming.
    - job_name: 'blackbox-on-prem-post'
      scrape_interval: 1m
      metrics_path: /probe
      params:
        module: [http_post_2xx]
      static_configs:
        - targets:
          - http://token.cloud.b2bgwsit.service.test/tokens
          labels:
            name: (OB) B2B DP Token SIT
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: prometheus-blackbox-exporter:9115
      scheme: https
      tls_config:
        ca_file: /etc/istio-certs/root-cert.pem
        cert_file: /etc/istio-certs/cert-chain.pem
        key_file: /etc/istio-certs/key.pem
        insecure_skip_verify: true  # prometheus does not support secure naming.
    - job_name: 'blackbox-cloudsql'
      scrape_interval: 1m
      metrics_path: /probe
      params:
        module: [tcp_connect]
      static_configs:
        - targets:
          - '10.190.0.3:5432'
          labels:
            name: (OB) Kong Eagle CloudSQL
            instance_id: kong-eagle-integration-api-gateway-526d87
        - targets:
          - '10.190.0.47:5432'
          labels:
            name: (OB) Kong Jackal CloudSQL
            instance_id: kong-jackal-integration-api-gateway-26bc99
        - targets:
          - '10.190.0.10:5432'
          labels:
            name: (OB) Kong Lion CloudSQL
            instance_id: kong-lion-integration-api-gateway-df872c
        - targets:
          - '10.190.0.41:5432'
          labels:
            name: (OB) Kong SIT CloudSQL
            instance_id: kong-sit-integration-api-gateway-9ff6dd
        - targets:
          - '10.190.0.93:5432'
          labels:
            name: (OB) BX Eagle CloudSQL
            instance_id: authdb-eagle-bx-afb144
        - targets:
          - '10.190.0.148:5432'
          labels:
            name: (OB) BX Jackal CloudSQL
            instance_id: authdb-jackal-bx-657022
        - targets:
          - '10.190.0.151:5432'
          labels:
            name: (OB) BX Lion CloudSQL
            instance_id: authdb-lion-bx-af14b8
        - targets:
          - '10.190.0.104:5432'
          labels:
            name: (OB) BX SIT CloudSQL
            instance_id: authdb-sit-bx-df5e26
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: prometheus-blackbox-exporter:9115
      scheme: https
      tls_config:
        ca_file: /etc/istio-certs/root-cert.pem
        cert_file: /etc/istio-certs/cert-chain.pem
        key_file: /etc/istio-certs/key.pem
        insecure_skip_verify: true  # prometheus does not support secure naming.
    - job_name: 'kube-state-metrics'
      static_configs:
      - targets: ['kube-state-metrics.ops-sre.svc.cluster.local:8080']
    
    alerting:
      alertmanagers:
      - kubernetes_sd_configs:
          - role: pod
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          regex: ops-sre
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app]
          regex: prometheus
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_component]
          regex: alertmanager
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          regex:
          action: drop
  rules: |
    groups:
    - name: k8s.rules
      rules:
      - expr: |
          sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""}[5m])) by (namespace)
        record: namespace:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          sum by (namespace, pod_name, container_name) (
            rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""}[5m])
          )
        record: namespace_pod_name_container_name:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          sum(container_memory_usage_bytes{job="kubelet", image!="", container_name!=""}) by (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: |
          sum by (namespace, pod_name) (
              sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""}[5m])) by (namespace, pod_name)
            * on (namespace, pod_name) group_left()
              label_replace(label_replace(kube_pod_labels{job="kube-state-metrics"}, "namespace", "$1", "exported_namespace", "(.*)"),"pod_name", "$1", "exported_pod", "(.*)")
          )
        record: namespace_name:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          sum by (namespace, pod_name) (
            sum(container_memory_usage_bytes{job="kubelet",image!="", container_name!=""}) by (pod_name, namespace)
          * on (namespace, pod_name) group_left()
            label_replace(label_replace(kube_pod_labels{job="kube-state-metrics"}, "namespace", "$1", "exported_namespace", "(.*)"),"pod_name", "$1", "exported_pod", "(.*)")
          )
        record: namespace_name:container_memory_usage_bytes:sum
      - expr: |
          sum by (namespace, label_name) (
            sum(kube_pod_container_resource_requests_memory_bytes{endpoints_name!=""} * on (endpoints_name, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"^(Pending|Running)$",endpoints_name!=""} == 1)) by (namespace, pod)
          * on (namespace, pod) group_left(label_name)
            label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod", "$1", "exported_pod", "(.*)")
          )
        record: namespace_name:kube_pod_container_resource_requests_memory_bytes:sum
      - expr: |
          sum by (namespace, label_name) (
            sum(kube_pod_container_resource_requests_cpu_cores{endpoints_name!=""} * on (endpoints_name, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"^(Pending|Running)$",endpoints_name!=""} == 1)) by (namespace, pod)
          * on (namespace, pod) group_left(label_name)
            label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod", "$1", "exported_pod", "(.*)")
          )
        record: namespace_name:kube_pod_container_resource_requests_cpu_cores:sum
      - expr: |
          sum(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          ) by (namespace, workload, pod)
        labels:
          workload_type: daemonset
        record: mixin_pod_workload
      - expr: |
          sum(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          ) by (namespace, workload, pod)
        labels:
          workload_type: statefulset
        record: mixin_pod_workload
    - name: kube-scheduler.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_binding_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_binding_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_binding_latency:histogram_quantile
    - name: kube-apiserver.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.99"
        record: cluster_quantile:apiserver_request_latencies:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.9"
        record: cluster_quantile:apiserver_request_latencies:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
        labels:
          quantile: "0.5"
        record: cluster_quantile:apiserver_request_latencies:histogram_quantile
    - name: node.rules
      rules:
      - expr: sum(min(kube_pod_info) by (node))
        record: ':kube_pod_info_node_count:'
      - expr: |
          max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
        record: 'node_namespace_pod:kube_pod_info:'
      - expr: |
          count by(node) (sum by(node, cpu) (
            node_cpu_seconds_total{job="node-exporter"}
             * on(node) group_left()
             node_namespace_pod:kube_pod_info:
          ))
        record: node:node_num_cpu:sum
      - expr: |
          1 - avg(rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m]))
        record: :node_cpu_utilisation:avg1m
      - expr: |
          1 - avg by (node) (
            rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m])
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:)
        record: node:node_cpu_utilisation:avg1m
      - expr: |
          node:node_cpu_utilisation:avg1m
            *
          node:node_num_cpu:sum
            /
          scalar(sum(node:node_num_cpu:sum))
        record: node:cluster_cpu_utilisation:ratio
      - expr: |
          sum(node_load1{job="node-exporter"})
          /
          sum(node:node_num_cpu:sum)
        record: ':node_cpu_saturation_load1:'
      - expr: |
          sum by (node) (
            node_load1{job="node-exporter"}
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:
          )
          /
          node:node_num_cpu:sum
        record: 'node:node_cpu_saturation_load1:'
      - expr: |
          1 -
          sum(node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
          /
          sum(node_memory_MemTotal_bytes{job="node-exporter"})
        record: ':node_memory_utilisation:'
      - expr: |
          sum(node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
        record: :node_memory_MemFreeCachedBuffers_bytes:sum
      - expr: |
          sum(node_memory_MemTotal_bytes{job="node-exporter"})
        record: :node_memory_MemTotal_bytes:sum
      - expr: |
          sum by (node) (
            (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
            * on (node) group_left()
              node_namespace_pod:kube_pod_info:
          )
        record: node:node_memory_bytes_available:sum
      - expr: |
          sum by (node) (
            node_memory_MemTotal_bytes{job="node-exporter"}
            * on (node) group_left()
              node_namespace_pod:kube_pod_info:
          )
        record: node:node_memory_bytes_total:sum
      - expr: |
          (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
          /
          node:node_memory_bytes_total:sum
        record: node:node_memory_utilisation:ratio
      - expr: |
          (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
          /
          scalar(sum(node:node_memory_bytes_total:sum))
        record: node:cluster_memory_utilisation:ratio
      - expr: |
          1e3 * sum(
            (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
            + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
          )
        record: :node_memory_swap_io_bytes:sum_rate
      - expr: |
          1 -
          sum by (node) (
            (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:
          )
          /
          sum by (node) (
            node_memory_MemTotal_bytes{job="node-exporter"}
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:
          )
        record: 'node:node_memory_utilisation:'
      - expr: |
          1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
        record: 'node:node_memory_utilisation_2:'
      - expr: |
          1e3 * sum by (node) (
            (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
            + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
            * on (node) group_left()
              node_namespace_pod:kube_pod_info:
          )
        record: node:node_memory_swap_io_bytes:sum_rate
      - expr: |
          avg(irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
        record: :node_disk_utilisation:avg_irate
      - expr: |
          avg by (node) (
            irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:
          )
        record: node:node_disk_utilisation:avg_irate
      - expr: |
          avg(irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
        record: :node_disk_saturation:avg_irate
      - expr: |
          avg by (node) (
            irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:
          )
        record: node:node_disk_saturation:avg_irate
      - expr: |
          max by (instance, namespace, pod, device) ((node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
          - node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
          / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
        record: 'node:node_filesystem_usage:'
      - expr: |
          max by (instance, namespace, pod, device) (node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"} / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
        record: 'node:node_filesystem_avail:'
      - expr: |
          sum(irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m])) +
          sum(irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
        record: :node_net_utilisation:sum_irate
      - expr: |
          sum by (node) (
            (irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m]) +
            irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:
          )
        record: node:node_net_utilisation:sum_irate
      - expr: |
          sum(irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m])) +
          sum(irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
        record: :node_net_saturation:sum_irate
      - expr: |
          sum by (node) (
            (irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m]) +
            irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
          * on (node) group_left()
            node_namespace_pod:kube_pod_info:
          )
        record: node:node_net_saturation:sum_irate
      - expr: |
          max(
            max(
              kube_pod_info{job="kube-state-metrics", host_ip!=""}
            ) by (node, host_ip)
            * on (host_ip) group_right (node)
            label_replace(
              (max(node_filesystem_files{job="node-exporter", mountpoint="/"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
            )
          ) by (node)
        record: 'node:node_inodes_total:'
      - expr: |
          max(
            max(
              kube_pod_info{job="kube-state-metrics", host_ip!=""}
            ) by (node, host_ip)
            * on (host_ip) group_right (node)
            label_replace(
              (max(node_filesystem_files_free{job="node-exporter", mountpoint="/"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
            )
          ) by (node)
        record: 'node:node_inodes_free:'
    - name: kube-prometheus-node-recording.rules
      rules:
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}))
          BY (instance)
        record: instance:node_filesystem_usage:sum
      - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
          (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total) BY
          (instance, cpu)) BY (instance)
        record: instance:node_cpu:ratio
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
          BY (instance, cpu))
        record: cluster:node_cpu:ratio
    
---
# Source: prometheus/templates/alertmanager-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-alertmanager
---
# Source: prometheus/templates/server-serviceaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus

---
# Source: prometheus/templates/alertmanager-clusterrole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-alertmanager
rules:

---
# Source: prometheus/templates/pushgateway-clusterrole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-pushgateway
rules:

---
# Source: prometheus/templates/server-clusterrole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get

---
# Source: prometheus/templates/alertmanager-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-alertmanager
subjects:
  - kind: ServiceAccount
    name: prometheus-alertmanager
    namespace: ops-sre
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-alertmanager

---
# Source: prometheus/templates/pushgateway-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-pushgateway
subjects:
  - kind: ServiceAccount
    name: default
    namespace: ops-sre
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-pushgateway

---
# Source: prometheus/templates/server-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: ops-sre
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus

---
# Source: prometheus/templates/alertmanager-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-alertmanager
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9093
  selector:
    component: "alertmanager"
    app: prometheus
    release: prometheus
  type: "ClusterIP"

---
# Source: prometheus/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9090"
    prometheus.io/scrape: "true"
    
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
  selector:
    component: "server"
    app: prometheus
    release: prometheus
  type: "ClusterIP"
---
# Source: prometheus/templates/alertmanager-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus-alertmanager
spec:
  selector:
    matchLabels:
      component: "alertmanager"
      app: prometheus
      release: prometheus
  replicas: 1
  strategy:
    type: Recreate
    
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        
      labels:
        component: "alertmanager"
        app: prometheus
        release: prometheus
        chart: prometheus-9.1.3
        heritage: Tiller
    spec:
      serviceAccountName: prometheus-alertmanager
      containers:
        - name: prometheus-alertmanager
          image: "asia.gcr.io/anz-cx-ob-build-np-de5569/prom/alertmanager:v0.20.0"
          imagePullPolicy: "Always"
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          args:
            - --config.file=/etc/config/alertmanager.yml
            - --storage.path=/data
            - --cluster.advertise-address=$(POD_IP):6783
            - --web.external-url=http://localhost:9093

          ports:
            - containerPort: 9093
          readinessProbe:
            httpGet:
              path: /#/status
              port: 9093
            initialDelaySeconds: 30
            timeoutSeconds: 30
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: "/data"
              subPath: ""

        - name: prometheus-alertmanager-configmap-reload
          image: "asia.gcr.io/anz-cx-ob-build-np-de5569/jimmidyson/configmap-reload:v0.3.0"
          imagePullPolicy: "Always"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9093/-/reload
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-alertmanager
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-alertmanager

---
# Source: prometheus/templates/server-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-9.1.3
    heritage: Tiller
  name: prometheus
spec:
  selector:
    matchLabels:
      component: "server"
      app: prometheus
      release: prometheus
  replicas: 1
  strategy:
    type: Recreate
    
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        
      labels:
        component: "server"
        app: prometheus
        release: prometheus
        chart: prometheus-9.1.3
        heritage: Tiller
    spec:
      serviceAccountName: prometheus
      containers:
        - name: prometheus-server-configmap-reload
          image: "asia.gcr.io/anz-cx-ob-build-np-de5569/jimmidyson/configmap-reload:v0.3.0"
          imagePullPolicy: "Always"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9090/-/reload
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "asia.gcr.io/anz-cx-ob-build-np-de5569/prom/prometheus:v2.11.1"
          imagePullPolicy: "Always"
          args:
            - --storage.tsdb.retention.time=720h
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
            - --web.enable-admin-api
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
            - name: server-label-mapping-volume
              mountPath: /mapping
              subPath: 
              readOnly: true
            
            - mountPath: /etc/istio-certs
              name: istio-certs
            
        - args:
          - --stackdriver.project-id=anz-cx-ob-stg-831a3b
          - --prometheus.wal-directory=/data/wal
          - --prometheus.api-address=http://127.0.0.1:9090
          - --stackdriver.kubernetes.location=australia-southeast1
          - --stackdriver.kubernetes.cluster-name=anz-cx-ob-stg-gke
          - --config-file=/mapping/prometheus-label-mapping.yaml
          - --include={job=~"kubernetes-pods"}
          - --log.level=debug
          image: asia.gcr.io/anz-cx-ob-build-np-de5569/stackdriver-prometheus/stackdriver-prometheus-sidecar:0.6.2
          imagePullPolicy: IfNotPresent
          name: stackdriver-prometheus-sidecar
          ports:
          - containerPort: 9091
            name: sidecar
          volumeMounts:
          - mountPath: /data
            name: storage-volume
          - mountPath: /mapping
            name: server-label-mapping-volume
        
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: prometheus
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus
        - name: istio-certs
          secret:
            defaultMode: 420
            optional: true
            secretName: istio.default
        
        - name: server-label-mapping-volume
          configMap:
            name: label-mapping
---
# Source: prometheus/templates/alertmanager-ingress.yaml

---
# Source: prometheus/templates/alertmanager-networkpolicy.yaml

---
# Source: prometheus/templates/alertmanager-podsecuritypolicy.yaml


---
# Source: prometheus/templates/alertmanager-pvc.yaml

---
# Source: prometheus/templates/alertmanager-service-headless.yaml


---
# Source: prometheus/templates/alertmanager-statefulset.yaml


---
# Source: prometheus/templates/kube-state-metrics-clusterrole.yaml


---
# Source: prometheus/templates/kube-state-metrics-clusterrolebinding.yaml

---
# Source: prometheus/templates/kube-state-metrics-deployment.yaml


---
# Source: prometheus/templates/kube-state-metrics-networkpolicy.yaml

---
# Source: prometheus/templates/kube-state-metrics-podsecuritypolicy.yaml


---
# Source: prometheus/templates/kube-state-metrics-serviceaccount.yaml

---
# Source: prometheus/templates/kube-state-metrics-svc.yaml


---
# Source: prometheus/templates/node-exporter-daemonset.yaml

---
# Source: prometheus/templates/node-exporter-podsecuritypolicy.yaml


---
# Source: prometheus/templates/node-exporter-role.yaml


---
# Source: prometheus/templates/node-exporter-rolebinding.yaml


---
# Source: prometheus/templates/node-exporter-service.yaml

---
# Source: prometheus/templates/node-exporter-serviceaccount.yaml

---
# Source: prometheus/templates/pushgateway-deployment.yaml


---
# Source: prometheus/templates/pushgateway-ingress.yaml

---
# Source: prometheus/templates/pushgateway-networkpolicy.yaml

---
# Source: prometheus/templates/pushgateway-podsecuritypolicy.yaml


---
# Source: prometheus/templates/pushgateway-pvc.yaml

---
# Source: prometheus/templates/pushgateway-service.yaml


---
# Source: prometheus/templates/pushgateway-serviceaccount.yaml

---
# Source: prometheus/templates/server-ingress.yaml

---
# Source: prometheus/templates/server-networkpolicy.yaml


---
# Source: prometheus/templates/server-podsecuritypolicy.yaml


---
# Source: prometheus/templates/server-pvc.yaml

---
# Source: prometheus/templates/server-service-headless.yaml

---
# Source: prometheus/templates/server-statefulset.yaml


